{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Libraries\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense , LSTM , Embedding , Conv1D , Bidirectional , GRU , Dropout\n",
    "from keras.layers import GlobalMaxPool1D, Dropout, Activation,CuDNNLSTM\n",
    "from keras.layers import MaxPooling1D, BatchNormalization,Conv2D,Flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "# Training and Test set processing\n",
    "train = pd.read_csv('../input/train.csv')\n",
    "test = pd.read_csv('../input/test.csv')\n",
    "\n",
    "embed_size = 300 # how big is each word vector\n",
    "max_features = 50000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 100 # max number of words in a question to use\n",
    "\n",
    "train_X = train[\"question_text\"].fillna(\"_na_\").values\n",
    "\n",
    "test_X = test[\"question_text\"].fillna(\"_na_\").values\n",
    "\n",
    "## Tokenize the sentences\n",
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train_X))\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "\n",
    "test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "## Pad the sentences \n",
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "## Get the target values\n",
    "train_y = train['target'].values\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "1e8b8d60c8b66b26789cea7dc8396daf9619584c"
   },
   "outputs": [],
   "source": [
    "\n",
    "#Using Embeddings\n",
    "embedding_index = dict()\n",
    "f = open('../input/embeddings/glove.840B.300d/glove.840B.300d.txt',encoding='utf8')\n",
    "\n",
    "for line in f:\n",
    "    values = line.split(\" \")\n",
    "    words = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embedding_index[words]= coefs\n",
    "    \n",
    "f.close()\n",
    "embedding_matrix = np.zeros((max_features, 300))\n",
    "for word, index in tokenizer.word_index.items():\n",
    "    if index > max_features - 1:\n",
    "        break\n",
    "    else:\n",
    "        embedding_vector = embedding_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            embedding_matrix[index] = embedding_vector\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "521d4ada2ea566c1da844f1c6b80b5939cfacd41"
   },
   "outputs": [],
   "source": [
    "# model\n",
    "'''\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=max_features, output_dim= embed_size , input_length=maxlen,weights=[embedding_matrix], trainable=False))\n",
    "model.add(Conv1D(64,3,strides=2,padding='same',activation='relu'))\n",
    "model.add(Bidirectional(GRU(128,activation='relu',dropout=0.25,recurrent_dropout=0.25)))\n",
    "model.add(Dropout(0.45))\n",
    "model.add(Dense(32,activation='relu'))\n",
    "model.add(Dense(1,activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "'''\n",
    "model=Sequential()\n",
    "model.add(Embedding(max_features, embed_size, weights=[embedding_matrix],input_length=maxlen,trainable = False))\n",
    "model.add(Bidirectional(CuDNNLSTM(128, return_sequences=True)))\n",
    "model.add(GlobalMaxPool1D())\n",
    "model.add(Dense(32, activation=\"relu\"))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "8440d94e2d2aa3d238bc8d757e24c0a28f157c74"
   },
   "outputs": [],
   "source": [
    "#training\n",
    "model.fit(train_X,train_y,epochs=2,batch_size=1024)\n",
    "print(\"Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e22d8a6f6f2088867380d570b72d62bf28201e46"
   },
   "outputs": [],
   "source": [
    "pred_test_y = model.predict([test_X], batch_size=1024, verbose=1)\n",
    "pred_test_y = np.where(pred_test_y>0.5,1,0)                                    #changing the threshold in this version\n",
    "out_df = pd.DataFrame({\"qid\":test[\"qid\"].values})\n",
    "out_df['prediction'] = pred_test_y\n",
    "out_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
